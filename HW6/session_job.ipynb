{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing records from Kafka to JDBC failed: An error occurred while calling o99.executeSql.\n",
      ": org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.events'.\n",
      "\n",
      "Table options are:\n",
      "\n",
      "'connector'='kafka'\n",
      "'format'='json'\n",
      "'properties.auto.offset.reset'='earliest'\n",
      "'properties.bootstrap.servers'='redpanda-1:29092'\n",
      "'scan.startup.mode'='earliest-offset'\n",
      "'topic'='green-trips'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:234)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:176)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:116)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4060)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2928)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2482)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2393)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2338)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:737)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:723)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3906)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryOrInList(SqlToRelConverter.java:1923)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertExists(SqlToRelConverter.java:1906)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.substituteSubQuery(SqlToRelConverter.java:1440)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.replaceSubQueries(SqlToRelConverter.java:1180)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.replaceSubQueries(SqlToRelConverter.java:1170)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertCollectionTable(SqlToRelConverter.java:2952)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2532)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2393)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2338)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:737)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:723)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3906)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:627)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:234)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:209)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:82)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:90)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:265)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNodeOrFail(SqlNodeToOperationConversion.java:375)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertSqlInsert(SqlNodeToOperationConversion.java:704)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:338)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:255)\n",
      "\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n",
      "\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:784)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kafka'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:683)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:657)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:230)\n",
      "\t... 46 more\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n",
      "\n",
      "Available factory identifiers are:\n",
      "\n",
      "blackhole\n",
      "datagen\n",
      "filesystem\n",
      "legacy-csv\n",
      "print\n",
      "python-arrow-source\n",
      "python-input-format\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:491)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:679)\n",
      "\t... 48 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.common.time import Duration\n",
    "\n",
    "def create_events_aggregated_sink(t_env):\n",
    "    table_name = 'processed_events_aggregated'\n",
    "    sink_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            PULocationID INT,\n",
    "            DOLocationID INT,\n",
    "            longest_streak BIGINT,\n",
    "            PRIMARY KEY (PULocationID, DOLocationID) NOT ENFORCED\n",
    "        ) WITH (\n",
    "            'connector' = 'jdbc',\n",
    "            'url' = 'jdbc:postgresql://postgres:5432/postgres',\n",
    "            'table-name' = '{table_name}',\n",
    "            'username' = 'postgres',\n",
    "            'password' = 'postgres',\n",
    "            'driver' = 'org.postgresql.Driver'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(sink_ddl)\n",
    "    return table_name\n",
    "\n",
    "def create_events_source_kafka(t_env):\n",
    "    table_name = \"events\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            lpep_pickup_datetime TIMESTAMP(3),\n",
    "            lpep_dropoff_datetime TIMESTAMP(3),\n",
    "            PULocationID INT,\n",
    "            DOLocationID INT,\n",
    "            passenger_count INT,\n",
    "            trip_distance FLOAT,\n",
    "            tip_amount FLOAT,\n",
    "            WATERMARK for lpep_dropoff_datetime as lpep_dropoff_datetime - INTERVAL '5' SECOND\n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'green-trips',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name\n",
    "\n",
    "def log_aggregation():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.enable_checkpointing(10 * 1000)\n",
    "    env.set_parallelism(3)\n",
    "\n",
    "    # Set up the table environment\n",
    "    settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "    t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "\n",
    "    watermark_strategy = (\n",
    "        WatermarkStrategy\n",
    "        .for_bounded_out_of_orderness(Duration.of_seconds(5))\n",
    "        .with_timestamp_assigner(\n",
    "            lambda event, timestamp: event[1]  # Usamos lpep_dropoff_datetime como el tiempo del evento\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create Kafka table\n",
    "        source_table = create_events_source_kafka(t_env)\n",
    "        aggregated_table = create_events_aggregated_sink(t_env)\n",
    "\n",
    "        t_env.execute_sql(f\"\"\"\n",
    "        INSERT INTO {aggregated_table}\n",
    "        SELECT\n",
    "            PULocationID,\n",
    "            DOLocationID,\n",
    "            COUNT(*) AS longest_streak\n",
    "        FROM TABLE(\n",
    "            SESSION(TABLE {source_table}, DESCRIPTOR(lpep_dropoff_datetime), INTERVAL '5' MINUTE)\n",
    "        )\n",
    "        GROUP BY PULocationID, DOLocationID;\n",
    "        \n",
    "        \"\"\").wait()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Writing records from Kafka to JDBC failed:\", str(e))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_aggregation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_events_source_kafka(t_env):\n",
    "    table_name = \"events\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            lpep_pickup_datetime TIMESTAMP(3),\n",
    "            lpep_dropoff_datetime TIMESTAMP(3),\n",
    "            PULocationID INT,\n",
    "            DOLocationID INT,\n",
    "            passenger_count INT,\n",
    "            trip_distance FLOAT,\n",
    "            tip_amount FLOAT,\n",
    "            WATERMARK for lpep_dropoff_datetime as lpep_dropoff_datetime - INTERVAL '5' SECOND\n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'green-trips',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json',\n",
    "            'json.ignore-parse-errors' = 'true'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
